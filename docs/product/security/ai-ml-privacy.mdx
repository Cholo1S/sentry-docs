---
title: "AI/ML: Protecting User Privacy"
sidebar_order: 10
description: "Learn about Sentry's approach to AI/ML privacy"
---

Throughout Sentry’s history, we’ve operated under a policy of [privacy by default](https://sentry.io/lp/privacy-by-default/). This same principal applies to our work in the Artificial Intelligence (AI) and Machine Learning (ML) space, where we want to be just as [transparent](https://blog.sentry.io/terms-of-service-update/) about what data we’re using and why.

Sentry is at a juncture where data and experience quality is much harder to maintain using traditional heuristics-based approaches. For example, fingerprinting error events as part of creating groups, has gotten a lot more complicated with the rise of JavaScript, and the use of extensions and third-party services. To train and validate models for grouping, notifications, and issue priority, Sentry will now need access to additional service data in order to be able to deliver a better user experience.

## General Consent

In view of the new data we need to collet to improve Sentry’s ability to train internal models (to better fingerprint errors and prioritize data within the Sentry system), we've added a new “Service Data Consent” agreement. You can find it by going to the **Settings** page in [Sentry](https://sentry.io) and clicking on the "Legal & Compliance" link under the "Usage & Billing" section.

<Note>
  While our model inputs will use customer data, the outputs will **never** risk
  exposing customer information.
</Note>

## Data We Access & How It's Used

We’re asking for access to the following forms of service data:

- Error messages
- Stack traces
- Spans
- DOM interactions

Our intended improvements will harness text embeddings – converting words into numerical values that capture their semantic information. In this context, we’ll treat embeddings as a set of features for a downstream task. For example, we may use this aggregated dataset to train a model that will be able to predict the severity of a new issue. **The information within these embeddings can't identify specific organizations, projects, or issues**. Moreover, our policy on data retention and deletion will apply to any data we leverage for AI features. This means that if you choose to delete any data in our production database, we'll also delete the corresponding numerical embeddings from our vector database.

## Generative AI

A subset of upcoming features will use Generative AI and Retrieval Augmented Generation (RAG). In this context, we'll use text embeddings as an index - a way to retrieve the most contextually relevant bits of source code (via our GitHub integration) or events for a particular Sentry issue. When it comes to RAG, embeddings will always be logically separated (never crossing customer boundaries).

We may use embeddings to provide improved context to our suggested fix feature, help with semantic search, or improve the relevance of the issue details page, but only for the specific customer whose service data those embeddings were derived from.

All functionality leveraging RAG will require user opt-in. If you don't want access to these new features, nothing will change.

## Data Handling

In addition to the consent mechanisms mentioned above:

1. We'll continue to encourage all customers to use our [various data scrubbing tools](https://docs.sentry.io/product/data-management-settings/scrubbing/) so that service data is sanitized before we receive it.
2. We'll apply the same deletion and retention rules to our training data as we do to the underlying service data. This means that if you delete service data, it will also be removed from our machine learning models automatically.
3. We'll scrub data for PII before it goes into any training set.
4. We'll ensure that the only service data presented in the output of any ML feature belongs to the customer using the feature.
5. We'll only use AI models built in-house or provided by our existing trusted [third-party sub-processors](https://sentry.io/legal/subprocessors/) who have made contractual commitments that are consistent with the above.

We're confident that with these controls in place, we'll be able to use service data to improve our products through AI while at the same time protecting that data.
